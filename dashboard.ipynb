{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import dash\n",
    "import flask\n",
    "import datetime\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "import regex\n",
    "import pickle\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import dash_table_experiments as dt\n",
    "import plotly.plotly as py\n",
    "import plotly.figure_factory as ff\n",
    "from dash.dependencies import Input, Output, State\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link-input container\n",
    "link_input = html.Div(\n",
    "    children=[\n",
    "        html.H3('Enter a product link...', style={'text-align': 'center'}),\n",
    "        html.Div(\n",
    "            children=[\n",
    "                dcc.Input(\n",
    "                    placeholder='Enter a product link...',\n",
    "                    id='link',\n",
    "                    value=None,\n",
    "                    type=\"text\",\n",
    "                    style={'width': '100%'})\n",
    "            ],\n",
    "            className='nine columns',\n",
    "            style={'margin-left': 2}),\n",
    "        html.Div(\n",
    "            children=[\n",
    "                html.Button(\n",
    "                    'Submit',\n",
    "                    type='button',\n",
    "                    id='submit-button',\n",
    "                    className='button-primary',\n",
    "                    style={'width': '100%'}),\n",
    "            ],\n",
    "            className='three columns',\n",
    "            style={'margin-left': 2}),\n",
    "    ],\n",
    "    className='row')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link-input container\n",
    "def create_info_container(df,image_src,sentiment):\n",
    "    return html.Div(\n",
    "        children=[\n",
    "            html.\n",
    "            Div(children=[\n",
    "                html.Img(\n",
    "                    id='image',\n",
    "                    src=\n",
    "                    image_src,\n",
    "                    style={\n",
    "                        'max-height': 200,\n",
    "                        'max-width': 200,\n",
    "                        'margin-left': 'auto',\n",
    "                        'margin-right': 'auto',\n",
    "                        'display': 'block'\n",
    "                    })\n",
    "            ],\n",
    "                className='three columns',\n",
    "                style={'margin': 5, 'max-height': 200, 'background-color': 'white'}),\n",
    "            html.Div(\n",
    "                children=[\n",
    "                    html.H4(df.iloc[0].article, style={'margin-top': 0, 'margin-bottom': 0}),\n",
    "                    html.Div(\n",
    "                        children=[\n",
    "                        html.Div(\n",
    "                            className='rating-bar',\n",
    "                            children=[\n",
    "                                html.Div(\n",
    "                                    className='rating',\n",
    "                                    style={\n",
    "                                        'width': str(sum(sentiment)/len(sentiment) * 100) + '%',\n",
    "                                        'height': 10\n",
    "                                    })\n",
    "                            ],\n",
    "                            style={'height': 10,'margin-left': 2,'max-width': 132}),\n",
    "                            html.Div(\n",
    "                                children=[],\n",
    "                                className='three columns',\n",
    "                                style={'margin-left': 2}),\n",
    "                        ],\n",
    "                        className='row')\n",
    "                ],\n",
    "                className='nine columns',\n",
    "                style={'margin-left': 10}),\n",
    "        ],\n",
    "        className='row', style={'margin-top': 40, 'borderRadius': '4px', 'background-color': 'lightgray'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_snippet(texts, highlights, lookahead):\n",
    "    snippets = []\n",
    "    #texts = texts.tolist()\n",
    "    #highlights = highlights.tolist()\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        highlight = highlights[i]\n",
    "        highlight_tokens = highlight.split(\" \")\n",
    "        tokens = text.split(\" \")\n",
    "        #tokens_x = x[i].split(\" \")# Split string into a list of tokens\n",
    "        tokens_x = text.lower()\n",
    "        #print(len(tokens))\n",
    "        #print(len(tokens_x))\n",
    "\n",
    "        found_index_start = tokens_x.index(highlight_tokens[0])\n",
    "        found_index_end = tokens_x.index(highlight_tokens[-1])\n",
    "\n",
    "        # get position of last char of found_index_end\n",
    "        k = 0\n",
    "        tmp_char = 'abc'\n",
    "        while tmp_char != ' ':\n",
    "            if found_index_end + k < len(tokens_x):\n",
    "                tmp_char = tokens_x[found_index_end + k]\n",
    "                k += 1\n",
    "            else:\n",
    "                tmp_char = ' '\n",
    "\n",
    "        try:\n",
    "            start_index = tokens_x.index(highlight_tokens[0])\n",
    "            end_index = tokens_x.index(highlight_tokens[-1]) + k\n",
    "            found_snippet = html.Span(\n",
    "                text[found_index_start:found_index_end + k],\n",
    "                style={'background-color': 'springgreen'})\n",
    "            start_snippet = \" \".join(\n",
    "                text[0:start_index].split(\" \")[-lookahead:])\n",
    "            end_snippet = \" \".join(text[end_index:-1].split(\" \")[:lookahead])\n",
    "            if lookahead < len(text[0:start_index].split(\" \")):\n",
    "                snippet = '...{}'.format(start_snippet)\n",
    "            if lookahead < len(text[end_index:-1].split(\" \")):\n",
    "                snippet = '{}...'.format(end_snippet)\n",
    "            snippet = html.Div(\n",
    "                [start_snippet, found_snippet, end_snippet])\n",
    "            snippets.append(snippet)\n",
    "\n",
    "        except ValueError:\n",
    "            snippet = \"\"  # No snippet or whatever error handling you are going to do\n",
    "\n",
    "    return snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_snippet(review, phrase, topic):\n",
    "    try:\n",
    "        review = review.lower()\n",
    "        tokens = review.split(\" \")\n",
    "        topic = topic.lower()\n",
    "        phrase_words = phrase.split()\n",
    "\n",
    "        found_indexes = []\n",
    "\n",
    "\n",
    "        for i, token in enumerate(tokens):\n",
    "            for k in range(len(phrase_words)):\n",
    "                matches = regex.compile('^'+phrase_words[k]+'[,]?$').search(token)\n",
    "                if matches:\n",
    "                    found_indexes.append(i)\n",
    "                    break\n",
    "\n",
    "        tokens2 = review.split(\" \")\n",
    "\n",
    "        children = []\n",
    "        tmp_string=''\n",
    "        for i, token2 in enumerate(tokens2):\n",
    "            if i in found_indexes:\n",
    "                if tmp_string != '':\n",
    "                    children.append(tmp_string)\n",
    "                    tmp_string = ' '\n",
    "                children.append(\n",
    "                html.Span(\n",
    "                token2,\n",
    "                style={'background-color': 'springgreen'}))\n",
    "            else:\n",
    "                tmp_string+=token2+' '\n",
    "        children.append(' '+tmp_string)\n",
    "\n",
    "        #end_snippet = \" \".join(text[end_index:-1].split(\" \")[:lookahead])\n",
    "        #if lookahead < len(text[0:start_index].split(\" \")):\n",
    "        #    snippet = '...{}'.format(start_snippet)\n",
    "        #if lookahead < len(text[end_index:-1].split(\" \")):\n",
    "        #    snippet = '{}...'.format(end_snippet)\n",
    "            \n",
    "        snippet = html.Div(children=children)\n",
    "\n",
    "    except ValueError:\n",
    "        snippet = \"\"  # No snippet or whatever error handling you are going to do\n",
    "\n",
    "    return snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_topics_container(sorted_topics,df_topic_scores):\n",
    "    topics_container = []\n",
    "    for topic in sorted_topics:\n",
    "        df_topic = df_topic_scores[df_topic_scores.topic == topic]\n",
    "        scores = df_topic.sentiment\n",
    "        topic_score = sum(scores)/len(scores)\n",
    "        #df_topic = df_topic.drop_duplicates(subset='phrase', keep=\"last\")\n",
    "        #df_topic = df_topic.reset_index()\n",
    "        df_neg = df_topic[df_topic.sentiment<=0.5]\n",
    "        df_pos = df_topic[df_topic.sentiment>0.5]\n",
    "\n",
    "        df_neg['final_score'] = df_neg.score*(1-df_neg.sentiment)\n",
    "        df_pos['final_score'] = df_pos.score*df_pos.sentiment\n",
    "        df_topic1 = df_pos.sort_values(by=['final_score'], ascending=False)\n",
    "        df_topic2 = df_neg.sort_values(by=['final_score'], ascending=False)\n",
    "\n",
    "            \n",
    "        if len(df_topic)>10:\n",
    "            fig = ff.create_distplot([scores.tolist()*5], group_labels = [''], bin_size=0.1, show_rug=False)\n",
    "            fig['layout'].update(showlegend=False, margin=go.layout.Margin(l=20,r=20,b=0,t=20,pad = 0), height=260)\n",
    "            distri = dcc.Graph(figure=fig, config={'staticPlot':True,'displayModeBar':False,'queueLength':0}, id='distribution-'+str(re.sub('[^a-zA-z0-9]', '', topic)))\n",
    "        else:\n",
    "            distri = None\n",
    "\n",
    "        phrases_pos =  [html.Div(children=[html.Div(children=[phrase,html.Button('Show', id='buttonPos-'+str(re.sub('[^a-zA-z0-9]', '', topic))+str(i), style={'border-left': '#555',\n",
    "    'border-top': 'None',\n",
    "    'border-bottom': 'None',\n",
    "    'border-right': 'None',\n",
    "    'border-width': '0.5px',\n",
    "    'border-left-style': 'solid',\n",
    "    'border-radius': 0,\n",
    "    'line-height': '0px',\n",
    "    'float': 'inline-end',\n",
    "    'height': 24})], style={'text-align': 'center', 'border-radius': 4, 'margin': 2, 'background-color': 'springgreen'}),html.Div(id='detailsPos-'+str(re.sub('[^a-zA-z0-9]', '', topic))+str(i), children=generate_snippet(df_topic1.drop_duplicates(subset='phrase', keep=\"last\").iloc[i].review, phrase, topic),\n",
    "                            style={\n",
    "\n",
    "                                'display': 'none'\n",
    "                            })]) for i, phrase in enumerate(df_topic1.drop_duplicates(subset='phrase', keep=\"last\")[:5].phrase)]\n",
    "        phrases_neg =  [html.Div(children=[html.Div(children=[phrase,html.Button('Show', id='buttonNeg-'+str(re.sub('[^a-zA-z0-9]', '', topic))+str(i), style={'border-left': '#555',\n",
    "    'border-top': 'None',\n",
    "    'border-bottom': 'None',\n",
    "    'border-right': 'None',\n",
    "    'border-width': '0.5px',\n",
    "    'border-left-style': 'solid',\n",
    "    'border-radius': 0,\n",
    "    'line-height': '0px',\n",
    "    'float': 'inline-end',\n",
    "    'height': 24})], style={'text-align': 'center', 'border-radius': 4, 'margin': 2, 'background-color': 'coral'}),html.Div(id='detailsNeg-'+str(re.sub('[^a-zA-z0-9]', '', topic))+str(i), children=generate_snippet(df_topic2.drop_duplicates(subset='phrase', keep=\"last\").iloc[i].review, phrase, topic),\n",
    "                            style={\n",
    "\n",
    "                                'display': 'none'\n",
    "                            })]) for i, phrase in enumerate(df_topic2.drop_duplicates(subset='phrase', keep=\"last\")[:5].phrase)]\n",
    "\n",
    "        # link-input container\n",
    "        topic_container = html.Div(\n",
    "            children=[\n",
    "                html.\n",
    "                Div(children=[\n",
    "                    html.H4(\n",
    "                        topic.title(), style={\n",
    "                            'margin-top': 0,\n",
    "                            'margin-bottom': 0\n",
    "                        }),\n",
    "                    html.Div(\n",
    "                        className='rating-bar',\n",
    "                        children=[\n",
    "                            html.Div(\n",
    "                                className='rating',\n",
    "                                style={\n",
    "                                    'width': str(topic_score * 100) + '%',\n",
    "                                    'height': 10\n",
    "                                })\n",
    "                        ],\n",
    "                        style={'height': 10}),\n",
    "                    html.Div(children=[\n",
    "                    html.Strong(\n",
    "                        '# Positive: ' + str(len(df_topic1)), style={\n",
    "                            'margin-top': 0,\n",
    "                            'margin-bottom': 0\n",
    "                        }),\n",
    "                    html.Br(),\n",
    "                    html.Strong(\n",
    "                        '# Negative: ' + str(len(df_topic2)), style={\n",
    "                            'margin-top': 0,\n",
    "                            'margin-bottom': 0\n",
    "                        }),\n",
    "                    html.Br(), \n",
    "                    html.Strong(\n",
    "                        '# Total: ' + str(len(df_topic1)+len(df_topic2)), style={\n",
    "                            'margin-top': 0,\n",
    "                            'margin-bottom': 0\n",
    "                        }),\n",
    "                    html.Br(),\n",
    "                    html.Strong(\n",
    "                        'Total share: ' + str(int((len(df_topic1)+len(df_topic2))/len(df_topic_scores)*100))+'%', style={\n",
    "                            'margin-top': 0,\n",
    "                            'margin-bottom': 0\n",
    "                        })  \n",
    "                        ],\n",
    "                    style={'margin-top': 20})\n",
    "                ],\n",
    "                    className='three columns',\n",
    "                    style={\n",
    "                        'margin-left': 2,\n",
    "                        'text-align': 'center'\n",
    "                    }),\n",
    "                html.Div(children=[\n",
    "                    *phrases_pos,\n",
    "                    *phrases_neg\n",
    "                ],\n",
    "                    className='five columns',\n",
    "                    style={'margin-left': 10}),\n",
    "                html.Div(children=[\n",
    "                    distri\n",
    "                ],\n",
    "                    className='four columns',\n",
    "                    style={'margin-left': 10})\n",
    "            ],\n",
    "            className='row',\n",
    "            style={'margin-top': 40})\n",
    "        topics_container.append(topic_container)\n",
    "        topics_container.append(html.Hr())\n",
    "    return topics_container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following cells contain the products used for our paper, you can choose one of them and then run the last cell in this block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitbit Alta HR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2306\n",
      "Create container...\n"
     ]
    }
   ],
   "source": [
    "global filename\n",
    "filename = './data/fitbitaltahrblacksmallusversion.json'\n",
    "global topics_filename\n",
    "topics_filename = './data/fitbitaltahrblacksmallusversion_topics.pkl'\n",
    "image_src = 'https://images-na.ssl-images-amazon.com/images/I/61vyVXBk9sL._SY679_.jpg'\n",
    "df = pd.read_pickle('./data/fitbitaltahrblacksmallusversion.pkl')\n",
    "score = [y for x in df.final_scores_with_sent for y in x]\n",
    "topic = [y for x in df.topics for y in x]\n",
    "phrase = [y for x in df.final_phrases_with_sent for y in x]\n",
    "score=[0 if x is None else x for x in score]\n",
    "sentiment = [y[1] for x in df.final_sent_scores2 for y in x]\n",
    "review = []\n",
    "for i, tmp_review in enumerate(df.review_body):\n",
    "    for phrase1 in df.final_phrases_with_sent.iloc[i]:\n",
    "        review.append(tmp_review)\n",
    "df_topic_scores = pd.DataFrame(data={'topic': topic, 'score': score, 'phrase': phrase,'review': review, 'sentiment': sentiment})\n",
    "df_topic_scores.dropna()\n",
    "print(len(df_topic_scores))\n",
    "df_topic_scores.drop_duplicates()\n",
    "#df_topic_scores = df_topic_scores[df_topic_scores.score<=0]\n",
    "with open(topics_filename, 'rb') as f:\n",
    "    sorted_topics = pickle.load(f)\n",
    "sorted_topics = [topic for topic in sorted_topics if topic in df_topic_scores.topic.unique()]\n",
    "print('Create container...')\n",
    "container = []\n",
    "container.append(create_info_container(df,image_src,sentiment))\n",
    "container2 = container+create_topics_container(sorted_topics,df_topic_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sony MDR7506:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7997\n",
      "Create container...\n"
     ]
    }
   ],
   "source": [
    "global filename\n",
    "filename = './data/sonymdr7506professionallargediaphragmheadphone.json'\n",
    "global topics_filename\n",
    "topics_filename = './data/sonymdr7506professionallargediaphragmheadphone_topics.pkl'\n",
    "image_src = 'https://images-na.ssl-images-amazon.com/images/I/81TzTAx8weL._SL1500_.jpg'\n",
    "df = pd.read_pickle('./data/sonymdr7506professionallargediaphragmheadphone.pkl')\n",
    "score = [y for x in df.final_scores_with_sent for y in x]\n",
    "topic = [y for x in df.topics for y in x]\n",
    "phrase = [y for x in df.final_phrases_with_sent for y in x]\n",
    "score=[0 if x is None else x for x in score]\n",
    "sentiment = [y[1] for x in df.final_sent_scores2 for y in x]\n",
    "review = []\n",
    "for i, tmp_review in enumerate(df.review_body):\n",
    "    for phrase1 in df.final_phrases_with_sent.iloc[i]:\n",
    "        review.append(tmp_review)\n",
    "df_topic_scores = pd.DataFrame(data={'topic': topic, 'score': score, 'phrase': phrase,'review': review, 'sentiment': sentiment})\n",
    "df_topic_scores.dropna()\n",
    "print(len(df_topic_scores))\n",
    "df_topic_scores.drop_duplicates()\n",
    "#df_topic_scores = df_topic_scores[df_topic_scores.score<=0]\n",
    "with open(topics_filename, 'rb') as f:\n",
    "    sorted_topics = pickle.load(f)\n",
    "sorted_topics = [topic for topic in sorted_topics if topic in df_topic_scores.topic.unique()]\n",
    "print('Create container...')\n",
    "container = []\n",
    "container.append(create_info_container(df,image_src,sentiment))\n",
    "container2 = container+create_topics_container(sorted_topics,df_topic_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sony MDR7506 with tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8191\n",
      "Create container...\n"
     ]
    }
   ],
   "source": [
    "global filename\n",
    "filename = './data/sonymdr7506professionallargediaphragmheadphone_tweets.json'\n",
    "global topics_filename\n",
    "topics_filename = './data/sonymdr7506professionallargediaphragmheadphone_tweets_topics.pkl'\n",
    "image_src = 'https://images-na.ssl-images-amazon.com/images/I/81TzTAx8weL._SL1500_.jpg'\n",
    "df = pd.read_pickle('./data/sonymdr7506professionallargediaphragmheadphone_tweets.pkl')\n",
    "score = [y for x in df.final_scores_with_sent for y in x]\n",
    "topic = [y for x in df.topics for y in x]\n",
    "phrase = [y for x in df.final_phrases_with_sent for y in x]\n",
    "score=[0 if x is None else x for x in score]\n",
    "sentiment = [y[1] for x in df.final_sent_scores2 for y in x]\n",
    "review = []\n",
    "for i, tmp_review in enumerate(df.review_body):\n",
    "    for phrase1 in df.final_phrases_with_sent.iloc[i]:\n",
    "        review.append(tmp_review)\n",
    "df_topic_scores = pd.DataFrame(data={'topic': topic, 'score': score, 'phrase': phrase,'review': review, 'sentiment': sentiment})\n",
    "df_topic_scores.dropna()\n",
    "print(len(df_topic_scores))\n",
    "df_topic_scores.drop_duplicates()\n",
    "#df_topic_scores = df_topic_scores[df_topic_scores.score<=0]\n",
    "with open(topics_filename, 'rb') as f:\n",
    "    sorted_topics = pickle.load(f)\n",
    "sorted_topics = [topic for topic in sorted_topics if topic in df_topic_scores.topic.unique()]\n",
    "print('Create container...')\n",
    "container = []\n",
    "container.append(create_info_container(df,image_src,sentiment))\n",
    "container2 = container+create_topics_container(sorted_topics,df_topic_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apple Iphone 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354\n",
      "Create container...\n"
     ]
    }
   ],
   "source": [
    "global filename\n",
    "filename = './data/appleiphone6sfullyunlocked64gbrosegoldrefurbished.json'\n",
    "global topics_filename\n",
    "topics_filename = './data/appleiphone6sfullyunlocked64gbrosegoldrefurbished_topics.pkl'\n",
    "image_src = 'https://images-na.ssl-images-amazon.com/images/I/81qiCrJlzgL._SL1500_.jpg'\n",
    "df = pd.read_pickle('./data/appleiphone6sfullyunlocked64gbrosegoldrefurbished.pkl')\n",
    "score = [y for x in df.final_scores_with_sent for y in x]\n",
    "topic = [y for x in df.topics for y in x]\n",
    "phrase = [y for x in df.final_phrases_with_sent for y in x]\n",
    "score=[0 if x is None else x for x in score]\n",
    "sentiment = [y[1] for x in df.final_sent_scores2 for y in x]\n",
    "review = []\n",
    "for i, tmp_review in enumerate(df.review_body):\n",
    "    for phrase1 in df.final_phrases_with_sent.iloc[i]:\n",
    "        review.append(tmp_review)\n",
    "df_topic_scores = pd.DataFrame(data={'topic': topic, 'score': score, 'phrase': phrase,'review': review, 'sentiment': sentiment})\n",
    "df_topic_scores.dropna()\n",
    "print(len(df_topic_scores))\n",
    "df_topic_scores.drop_duplicates()\n",
    "#df_topic_scores = df_topic_scores[df_topic_scores.score<=0]\n",
    "with open(topics_filename, 'rb') as f:\n",
    "    sorted_topics = pickle.load(f)\n",
    "sorted_topics = [topic for topic in sorted_topics if topic in df_topic_scores.topic.unique()]\n",
    "print('Create container...')\n",
    "container = []\n",
    "container.append(create_info_container(df,image_src,sentiment))\n",
    "container2 = container+create_topics_container(sorted_topics,df_topic_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitbit Flex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4042\n",
      "Create container...\n"
     ]
    }
   ],
   "source": [
    "global filename\n",
    "filename = './data/fitbitflexwirelessactivitysleepwristbandblack.json'\n",
    "global topics_filename\n",
    "topics_filename = './data/fitbitflexwirelessactivitysleepwristbandblack_topics.pkl'\n",
    "image_src = 'https://images-na.ssl-images-amazon.com/images/I/81qiCrJlzgL._SL1500_.jpg'\n",
    "df = pd.read_pickle('./data/fitbitflexwirelessactivitysleepwristbandblack.pkl')\n",
    "score = [y for x in df.final_scores_with_sent for y in x]\n",
    "topic = [y for x in df.topics for y in x]\n",
    "phrase = [y for x in df.final_phrases_with_sent for y in x]\n",
    "score=[0 if x is None else x for x in score]\n",
    "sentiment = [y[1] for x in df.final_sent_scores2 for y in x]\n",
    "review = []\n",
    "for i, tmp_review in enumerate(df.review_body):\n",
    "    for phrase1 in df.final_phrases_with_sent.iloc[i]:\n",
    "        review.append(tmp_review)\n",
    "df_topic_scores = pd.DataFrame(data={'topic': topic, 'score': score, 'phrase': phrase,'review': review, 'sentiment': sentiment})\n",
    "df_topic_scores.dropna()\n",
    "print(len(df_topic_scores))\n",
    "df_topic_scores.drop_duplicates()\n",
    "#df_topic_scores = df_topic_scores[df_topic_scores.score<=0]\n",
    "with open(topics_filename, 'rb') as f:\n",
    "    sorted_topics = pickle.load(f)\n",
    "sorted_topics = [topic for topic in sorted_topics if topic in df_topic_scores.topic.unique()]\n",
    "print('Create container...')\n",
    "container = []\n",
    "container.append(create_info_container(df,image_src,sentiment))\n",
    "container2 = container+create_topics_container(sorted_topics,df_topic_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create callbacks...\n",
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: Do not use the development server in a production environment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://0.0.0.0:8050/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:19] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:19] \"\u001b[37mGET /_dash-component-suites/dash_renderer/react-dom@15.4.2.min.js?v=0.15.0&m=1542374987 HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:19] \"\u001b[37mGET /_dash-component-suites/dash_renderer/react@15.4.2.min.js?v=0.15.0&m=1542374987 HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:19] \"\u001b[37mGET /_dash-component-suites/dash_html_components/dash_html_components.min.js?v=0.13.2&m=1542375024 HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:20] \"\u001b[37mGET /_dash-dependencies HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:20] \"\u001b[37mGET /_dash-layout HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:23] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:23] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:23] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:23] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:23] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:23] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:23] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:23] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:23] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:23] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:23] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:09:23] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:11:04] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:11:09] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:25:26] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:25:44] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2019 16:46:33] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n",
    "\n",
    "# create app\n",
    "app = dash.Dash(__name__, external_stylesheets=external_stylesheets)\n",
    "app.css.append_css({\"external_url\": \"https://codepen.io/anon/pen/MzZQwp.css\"})\n",
    "app.css.append_css({\n",
    "    \"external_url\":\n",
    "    \"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\"\n",
    "})\n",
    "app.css.append_css({\n",
    "    \"external_url\":\n",
    "    \"https://codepen.io/austingreen/pen/burBc.css\"\n",
    "})\n",
    "app.scripts.config.serve_locally = True\n",
    "app.layout = html.Div([\n",
    "    link_input, *container2\n",
    "])\n",
    "app.config['suppress_callback_exceptions']=True\n",
    "\n",
    "print('Create callbacks...')\n",
    "inputs = []\n",
    "outputs = []\n",
    "for topic in sorted_topics:\n",
    "    scores = df_topic_scores[df_topic_scores.topic == topic].score\n",
    "    topic_score = (scores-min(scores))/(max(scores)-min(scores))\n",
    "    df_topic = df_topic_scores[df_topic_scores.topic == topic]\n",
    "    df_topic = df_topic.drop_duplicates(subset='phrase', keep=\"last\")\n",
    "    df_topic = df_topic.reset_index()\n",
    "    df_neg = df_topic[df_topic.sentiment<=0.5]\n",
    "    df_pos = df_topic[df_topic.sentiment>=0.5]\n",
    "\n",
    "    df_topic1 = df_pos.sort_values(by=['score'], ascending=False)\n",
    "    df_topic2 = df_neg.sort_values(by=['score'], ascending=True)\n",
    "    if len(df_topic1)>5:\n",
    "        max_len_topic1=5\n",
    "    else:\n",
    "        max_len_topic1=len(df_topic1)\n",
    "    if len(df_topic2)>5:\n",
    "        max_len_topic2=5\n",
    "    else:\n",
    "        max_len_topic2=len(df_topic2)\n",
    "    for i in range(max_len_topic1):\n",
    "        inputs.append(Input('buttonPos-'+str(re.sub('[^a-zA-z0-9]', '', topic))+str(i), 'n_clicks'))\n",
    "        outputs.append(Output('detailsPos-'+str(re.sub('[^a-zA-z0-9]', '', topic))+str(i), 'style'))\n",
    "    for i in range(max_len_topic2):\n",
    "        inputs.append(Input('buttonNeg-'+str(re.sub('[^a-zA-z0-9]', '', topic))+str(i), 'n_clicks'))\n",
    "        outputs.append(Output('detailsNeg-'+str(re.sub('[^a-zA-z0-9]', '', topic))+str(i), 'style'))\n",
    "\n",
    "def radio_toggle(n_clicks):\n",
    "    if n_clicks:\n",
    "        if n_clicks %2 != 0:\n",
    "            return {'display': 'block',\n",
    "                   'border-radius': 4,\n",
    "                    'margin': 2,\n",
    "                    'background-color': 'lightgrey',\n",
    "                    'padding': 5}\n",
    "        else:\n",
    "            return {'display': 'none'}\n",
    "    else:\n",
    "        return {'display': 'none'}\n",
    "    \n",
    "for i, output in enumerate(outputs):\n",
    "    app.callback(output, [inputs[i]])(radio_toggle)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create callbacks \n",
    "\n",
    "# run app localy\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=False, host='0.0.0.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the dynamic version of the dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: Do not use the development server in a production environment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://0.0.0.0:8050/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [06/Mar/2019 11:03:55] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2019 11:03:57] \"\u001b[37mGET /_dash-layout HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2019 11:03:57] \"\u001b[37mGET /_dash-dependencies HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2019 11:03:57] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/dp/B01M01ZZAC/ref=sr_1_4?keywords=google+phone&qid=1551866661&s=gateway&sr=8-4\n",
      "Load methods...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-06 11:05:03 [scrapy.crawler] INFO: Overridden settings: {'CONCURRENT_REQUESTS': '1', 'DOWNLOAD_DELAY': '2'}\n",
      "2019-03-06 11:05:03 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage']\n",
      "2019-03-06 11:05:03 [py.warnings] WARNING: /home/timon/miniconda3/envs/textmining/lib/python3.5/site-packages/scrapy/core/downloader/webclient.py:4: DeprecationWarning:\n",
      "\n",
      "twisted.web.client.HTTPClientFactory was deprecated in Twisted 16.7.0: please use https://pypi.org/project/treq/ or twisted.web.client.Agent instead\n",
      "\n",
      "\n",
      "2019-03-06 11:05:03 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2019-03-06 11:05:03 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2019-03-06 11:05:03 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2019-03-06 11:05:03 [scrapy.core.engine] INFO: Spider opened\n",
      "2019-03-06 11:05:03 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2019-03-06 11:05:03 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n",
      "2019-03-06 11:05:03 [twisted] CRITICAL: Unhandled Error\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/timon/miniconda3/envs/textmining/lib/python3.5/site-packages/twisted/internet/base.py\", line 428, in fireEvent\n",
      "    DeferredList(beforeResults).addCallback(self._continueFiring)\n",
      "  File \"/home/timon/miniconda3/envs/textmining/lib/python3.5/site-packages/twisted/internet/defer.py\", line 322, in addCallback\n",
      "    callbackKeywords=kw)\n",
      "  File \"/home/timon/miniconda3/envs/textmining/lib/python3.5/site-packages/twisted/internet/defer.py\", line 311, in addCallbacks\n",
      "    self._runCallbacks()\n",
      "  File \"/home/timon/miniconda3/envs/textmining/lib/python3.5/site-packages/twisted/internet/defer.py\", line 654, in _runCallbacks\n",
      "    current.result = callback(current.result, *args, **kw)\n",
      "--- <exception caught here> ---\n",
      "  File \"/home/timon/miniconda3/envs/textmining/lib/python3.5/site-packages/twisted/internet/base.py\", line 441, in _continueFiring\n",
      "    callable(*args, **kwargs)\n",
      "  File \"/home/timon/miniconda3/envs/textmining/lib/python3.5/site-packages/twisted/internet/base.py\", line 1256, in _reallyStartRunning\n",
      "    self._handleSignals()\n",
      "  File \"/home/timon/miniconda3/envs/textmining/lib/python3.5/site-packages/twisted/internet/posixbase.py\", line 295, in _handleSignals\n",
      "    _SignalReactorMixin._handleSignals(self)\n",
      "  File \"/home/timon/miniconda3/envs/textmining/lib/python3.5/site-packages/twisted/internet/base.py\", line 1221, in _handleSignals\n",
      "    signal.signal(signal.SIGINT, self.sigInt)\n",
      "  File \"/home/timon/miniconda3/envs/textmining/lib/python3.5/signal.py\", line 47, in signal\n",
      "    handler = _signal.signal(_enum_to_int(signalnum), _enum_to_int(handler))\n",
      "builtins.ValueError: signal only works in main thread\n",
      "\n",
      "2019-03-06 11:05:03 [py.warnings] WARNING: /home/timon/miniconda3/envs/textmining/lib/python3.5/site-packages/scrapy/core/downloader/contextfactory.py:51: DeprecationWarning:\n",
      "\n",
      "Passing method to twisted.internet.ssl.CertificateOptions was deprecated in Twisted 17.1.0. Please use a combination of insecurelyLowerMinimumTo, raiseMinimumTo, and lowerMaximumSecurityTo instead, as Twisted will correctly configure the method.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-06 11:05:04 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/dp/B01M01ZZAC> from <GET https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/dp/B01M01ZZAC/ref=sr_1_4?keywords=google+phone&qid=1551866661&s=gateway&sr=8-4>\n",
      "2019-03-06 11:05:05 [py.warnings] WARNING: /home/timon/miniconda3/envs/textmining/lib/python3.5/site-packages/scrapy/core/downloader/contextfactory.py:51: DeprecationWarning:\n",
      "\n",
      "Passing method to twisted.internet.ssl.CertificateOptions was deprecated in Twisted 17.1.0. Please use a combination of insecurelyLowerMinimumTo, raiseMinimumTo, and lowerMaximumSecurityTo instead, as Twisted will correctly configure the method.\n",
      "\n",
      "\n",
      "2019-03-06 11:05:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/dp/B01M01ZZAC> (referer: None)\n",
      "2019-03-06 11:05:07 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2019-03-06 11:05:07 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 589,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 131130,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'downloader/response_status_count/301': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2019, 3, 6, 10, 5, 7, 856745),\n",
      " 'log_count/CRITICAL': 1,\n",
      " 'log_count/DEBUG': 3,\n",
      " 'log_count/INFO': 7,\n",
      " 'log_count/WARNING': 3,\n",
      " 'memusage/max': 4103401472,\n",
      " 'memusage/startup': 4103401472,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2019, 3, 6, 10, 5, 3, 899555)}\n",
      "2019-03-06 11:05:07 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2019-03-06 11:05:07 [scrapy.crawler] INFO: Overridden settings: {'CONCURRENT_REQUESTS': '1', 'DOWNLOAD_DELAY': '2'}\n",
      "2019-03-06 11:05:07 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage']\n",
      "2019-03-06 11:05:07 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2019-03-06 11:05:07 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2019-03-06 11:05:07 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2019-03-06 11:05:07 [scrapy.core.engine] INFO: Spider opened\n",
      "2019-03-06 11:05:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2019-03-06 11:05:07 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n",
      "2019-03-06 11:05:07 [py.warnings] WARNING: /home/timon/miniconda3/envs/textmining/lib/python3.5/site-packages/scrapy/core/downloader/contextfactory.py:51: DeprecationWarning:\n",
      "\n",
      "Passing method to twisted.internet.ssl.CertificateOptions was deprecated in Twisted 17.1.0. Please use a combination of insecurelyLowerMinimumTo, raiseMinimumTo, and lowerMaximumSecurityTo instead, as Twisted will correctly configure the method.\n",
      "\n",
      "\n",
      "2019-03-06 11:05:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?reviewerType=all_reviews&pageNumber=1> (referer: None)\n",
      "2019-03-06 11:05:10 [py.warnings] WARNING: /home/timon/miniconda3/envs/textmining/lib/python3.5/site-packages/scrapy/core/downloader/contextfactory.py:51: DeprecationWarning:\n",
      "\n",
      "Passing method to twisted.internet.ssl.CertificateOptions was deprecated in Twisted 17.1.0. Please use a combination of insecurelyLowerMinimumTo, raiseMinimumTo, and lowerMaximumSecurityTo instead, as Twisted will correctly configure the method.\n",
      "\n",
      "\n",
      "2019-03-06 11:05:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=2&reviewerType=all_reviews> (referer: https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?reviewerType=all_reviews&pageNumber=1)\n",
      "2019-03-06 11:05:12 [py.warnings] WARNING: /home/timon/miniconda3/envs/textmining/lib/python3.5/site-packages/scrapy/core/downloader/contextfactory.py:51: DeprecationWarning:\n",
      "\n",
      "Passing method to twisted.internet.ssl.CertificateOptions was deprecated in Twisted 17.1.0. Please use a combination of insecurelyLowerMinimumTo, raiseMinimumTo, and lowerMaximumSecurityTo instead, as Twisted will correctly configure the method.\n",
      "\n",
      "\n",
      "2019-03-06 11:05:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=3&reviewerType=all_reviews> (referer: https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=2&reviewerType=all_reviews)\n",
      "2019-03-06 11:05:14 [py.warnings] WARNING: /home/timon/miniconda3/envs/textmining/lib/python3.5/site-packages/scrapy/core/downloader/contextfactory.py:51: DeprecationWarning:\n",
      "\n",
      "Passing method to twisted.internet.ssl.CertificateOptions was deprecated in Twisted 17.1.0. Please use a combination of insecurelyLowerMinimumTo, raiseMinimumTo, and lowerMaximumSecurityTo instead, as Twisted will correctly configure the method.\n",
      "\n",
      "\n",
      "2019-03-06 11:05:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=4&reviewerType=all_reviews> (referer: https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=3&reviewerType=all_reviews)\n",
      "2019-03-06 11:05:16 [py.warnings] WARNING: /home/timon/miniconda3/envs/textmining/lib/python3.5/site-packages/scrapy/core/downloader/contextfactory.py:51: DeprecationWarning:\n",
      "\n",
      "Passing method to twisted.internet.ssl.CertificateOptions was deprecated in Twisted 17.1.0. Please use a combination of insecurelyLowerMinimumTo, raiseMinimumTo, and lowerMaximumSecurityTo instead, as Twisted will correctly configure the method.\n",
      "\n",
      "\n",
      "2019-03-06 11:05:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=5&reviewerType=all_reviews> (referer: https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=4&reviewerType=all_reviews)\n",
      "2019-03-06 11:05:19 [py.warnings] WARNING: /home/timon/miniconda3/envs/textmining/lib/python3.5/site-packages/scrapy/core/downloader/contextfactory.py:51: DeprecationWarning:\n",
      "\n",
      "Passing method to twisted.internet.ssl.CertificateOptions was deprecated in Twisted 17.1.0. Please use a combination of insecurelyLowerMinimumTo, raiseMinimumTo, and lowerMaximumSecurityTo instead, as Twisted will correctly configure the method.\n",
      "\n",
      "\n",
      "2019-03-06 11:05:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=6&reviewerType=all_reviews> (referer: https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=5&reviewerType=all_reviews)\n",
      "2019-03-06 11:05:21 [py.warnings] WARNING: /home/timon/miniconda3/envs/textmining/lib/python3.5/site-packages/scrapy/core/downloader/contextfactory.py:51: DeprecationWarning:\n",
      "\n",
      "Passing method to twisted.internet.ssl.CertificateOptions was deprecated in Twisted 17.1.0. Please use a combination of insecurelyLowerMinimumTo, raiseMinimumTo, and lowerMaximumSecurityTo instead, as Twisted will correctly configure the method.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-06 11:05:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=7&reviewerType=all_reviews> (referer: https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=6&reviewerType=all_reviews)\n",
      "2019-03-06 11:05:24 [py.warnings] WARNING: /home/timon/miniconda3/envs/textmining/lib/python3.5/site-packages/scrapy/core/downloader/contextfactory.py:51: DeprecationWarning:\n",
      "\n",
      "Passing method to twisted.internet.ssl.CertificateOptions was deprecated in Twisted 17.1.0. Please use a combination of insecurelyLowerMinimumTo, raiseMinimumTo, and lowerMaximumSecurityTo instead, as Twisted will correctly configure the method.\n",
      "\n",
      "\n",
      "2019-03-06 11:05:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=8&reviewerType=all_reviews> (referer: https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=7&reviewerType=all_reviews)\n",
      "2019-03-06 11:05:27 [py.warnings] WARNING: /home/timon/miniconda3/envs/textmining/lib/python3.5/site-packages/scrapy/core/downloader/contextfactory.py:51: DeprecationWarning:\n",
      "\n",
      "Passing method to twisted.internet.ssl.CertificateOptions was deprecated in Twisted 17.1.0. Please use a combination of insecurelyLowerMinimumTo, raiseMinimumTo, and lowerMaximumSecurityTo instead, as Twisted will correctly configure the method.\n",
      "\n",
      "\n",
      "2019-03-06 11:05:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=9&reviewerType=all_reviews> (referer: https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=8&reviewerType=all_reviews)\n",
      "2019-03-06 11:05:29 [py.warnings] WARNING: /home/timon/miniconda3/envs/textmining/lib/python3.5/site-packages/scrapy/core/downloader/contextfactory.py:51: DeprecationWarning:\n",
      "\n",
      "Passing method to twisted.internet.ssl.CertificateOptions was deprecated in Twisted 17.1.0. Please use a combination of insecurelyLowerMinimumTo, raiseMinimumTo, and lowerMaximumSecurityTo instead, as Twisted will correctly configure the method.\n",
      "\n",
      "\n",
      "2019-03-06 11:05:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=10&reviewerType=all_reviews> (referer: https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=9&reviewerType=all_reviews)\n",
      "2019-03-06 11:05:32 [py.warnings] WARNING: /home/timon/miniconda3/envs/textmining/lib/python3.5/site-packages/scrapy/core/downloader/contextfactory.py:51: DeprecationWarning:\n",
      "\n",
      "Passing method to twisted.internet.ssl.CertificateOptions was deprecated in Twisted 17.1.0. Please use a combination of insecurelyLowerMinimumTo, raiseMinimumTo, and lowerMaximumSecurityTo instead, as Twisted will correctly configure the method.\n",
      "\n",
      "\n",
      "2019-03-06 11:05:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=11&reviewerType=all_reviews> (referer: https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=10&reviewerType=all_reviews)\n",
      "2019-03-06 11:05:34 [py.warnings] WARNING: /home/timon/miniconda3/envs/textmining/lib/python3.5/site-packages/scrapy/core/downloader/contextfactory.py:51: DeprecationWarning:\n",
      "\n",
      "Passing method to twisted.internet.ssl.CertificateOptions was deprecated in Twisted 17.1.0. Please use a combination of insecurelyLowerMinimumTo, raiseMinimumTo, and lowerMaximumSecurityTo instead, as Twisted will correctly configure the method.\n",
      "\n",
      "\n",
      "2019-03-06 11:05:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=12&reviewerType=all_reviews> (referer: https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=11&reviewerType=all_reviews)\n",
      "2019-03-06 11:05:37 [py.warnings] WARNING: /home/timon/miniconda3/envs/textmining/lib/python3.5/site-packages/scrapy/core/downloader/contextfactory.py:51: DeprecationWarning:\n",
      "\n",
      "Passing method to twisted.internet.ssl.CertificateOptions was deprecated in Twisted 17.1.0. Please use a combination of insecurelyLowerMinimumTo, raiseMinimumTo, and lowerMaximumSecurityTo instead, as Twisted will correctly configure the method.\n",
      "\n",
      "\n",
      "2019-03-06 11:05:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=13&reviewerType=all_reviews> (referer: https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=12&reviewerType=all_reviews)\n",
      "2019-03-06 11:05:39 [py.warnings] WARNING: /home/timon/miniconda3/envs/textmining/lib/python3.5/site-packages/scrapy/core/downloader/contextfactory.py:51: DeprecationWarning:\n",
      "\n",
      "Passing method to twisted.internet.ssl.CertificateOptions was deprecated in Twisted 17.1.0. Please use a combination of insecurelyLowerMinimumTo, raiseMinimumTo, and lowerMaximumSecurityTo instead, as Twisted will correctly configure the method.\n",
      "\n",
      "\n",
      "2019-03-06 11:05:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=14&reviewerType=all_reviews> (referer: https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=13&reviewerType=all_reviews)\n",
      "2019-03-06 11:05:41 [py.warnings] WARNING: /home/timon/miniconda3/envs/textmining/lib/python3.5/site-packages/scrapy/core/downloader/contextfactory.py:51: DeprecationWarning:\n",
      "\n",
      "Passing method to twisted.internet.ssl.CertificateOptions was deprecated in Twisted 17.1.0. Please use a combination of insecurelyLowerMinimumTo, raiseMinimumTo, and lowerMaximumSecurityTo instead, as Twisted will correctly configure the method.\n",
      "\n",
      "\n",
      "2019-03-06 11:05:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=15&reviewerType=all_reviews> (referer: https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=14&reviewerType=all_reviews)\n",
      "2019-03-06 11:05:44 [py.warnings] WARNING: /home/timon/miniconda3/envs/textmining/lib/python3.5/site-packages/scrapy/core/downloader/contextfactory.py:51: DeprecationWarning:\n",
      "\n",
      "Passing method to twisted.internet.ssl.CertificateOptions was deprecated in Twisted 17.1.0. Please use a combination of insecurelyLowerMinimumTo, raiseMinimumTo, and lowerMaximumSecurityTo instead, as Twisted will correctly configure the method.\n",
      "\n",
      "\n",
      "2019-03-06 11:05:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=16&reviewerType=all_reviews> (referer: https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=15&reviewerType=all_reviews)\n",
      "2019-03-06 11:05:47 [py.warnings] WARNING: /home/timon/miniconda3/envs/textmining/lib/python3.5/site-packages/scrapy/core/downloader/contextfactory.py:51: DeprecationWarning:\n",
      "\n",
      "Passing method to twisted.internet.ssl.CertificateOptions was deprecated in Twisted 17.1.0. Please use a combination of insecurelyLowerMinimumTo, raiseMinimumTo, and lowerMaximumSecurityTo instead, as Twisted will correctly configure the method.\n",
      "\n",
      "\n",
      "2019-03-06 11:05:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=17&reviewerType=all_reviews> (referer: https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=16&reviewerType=all_reviews)\n",
      "2019-03-06 11:05:50 [py.warnings] WARNING: /home/timon/miniconda3/envs/textmining/lib/python3.5/site-packages/scrapy/core/downloader/contextfactory.py:51: DeprecationWarning:\n",
      "\n",
      "Passing method to twisted.internet.ssl.CertificateOptions was deprecated in Twisted 17.1.0. Please use a combination of insecurelyLowerMinimumTo, raiseMinimumTo, and lowerMaximumSecurityTo instead, as Twisted will correctly configure the method.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-06 11:05:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=18&reviewerType=all_reviews> (referer: https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=17&reviewerType=all_reviews)\n",
      "2019-03-06 11:05:52 [py.warnings] WARNING: /home/timon/miniconda3/envs/textmining/lib/python3.5/site-packages/scrapy/core/downloader/contextfactory.py:51: DeprecationWarning:\n",
      "\n",
      "Passing method to twisted.internet.ssl.CertificateOptions was deprecated in Twisted 17.1.0. Please use a combination of insecurelyLowerMinimumTo, raiseMinimumTo, and lowerMaximumSecurityTo instead, as Twisted will correctly configure the method.\n",
      "\n",
      "\n",
      "2019-03-06 11:05:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=19&reviewerType=all_reviews> (referer: https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=18&reviewerType=all_reviews)\n",
      "2019-03-06 11:05:55 [py.warnings] WARNING: /home/timon/miniconda3/envs/textmining/lib/python3.5/site-packages/scrapy/core/downloader/contextfactory.py:51: DeprecationWarning:\n",
      "\n",
      "Passing method to twisted.internet.ssl.CertificateOptions was deprecated in Twisted 17.1.0. Please use a combination of insecurelyLowerMinimumTo, raiseMinimumTo, and lowerMaximumSecurityTo instead, as Twisted will correctly configure the method.\n",
      "\n",
      "\n",
      "2019-03-06 11:05:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=20&reviewerType=all_reviews> (referer: https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=19&reviewerType=all_reviews)\n",
      "2019-03-06 11:05:58 [py.warnings] WARNING: /home/timon/miniconda3/envs/textmining/lib/python3.5/site-packages/scrapy/core/downloader/contextfactory.py:51: DeprecationWarning:\n",
      "\n",
      "Passing method to twisted.internet.ssl.CertificateOptions was deprecated in Twisted 17.1.0. Please use a combination of insecurelyLowerMinimumTo, raiseMinimumTo, and lowerMaximumSecurityTo instead, as Twisted will correctly configure the method.\n",
      "\n",
      "\n",
      "2019-03-06 11:05:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=21&reviewerType=all_reviews> (referer: https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=20&reviewerType=all_reviews)\n",
      "2019-03-06 11:06:01 [py.warnings] WARNING: /home/timon/miniconda3/envs/textmining/lib/python3.5/site-packages/scrapy/core/downloader/contextfactory.py:51: DeprecationWarning:\n",
      "\n",
      "Passing method to twisted.internet.ssl.CertificateOptions was deprecated in Twisted 17.1.0. Please use a combination of insecurelyLowerMinimumTo, raiseMinimumTo, and lowerMaximumSecurityTo instead, as Twisted will correctly configure the method.\n",
      "\n",
      "\n",
      "2019-03-06 11:06:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=22&reviewerType=all_reviews> (referer: https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=21&reviewerType=all_reviews)\n",
      "2019-03-06 11:06:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=22&reviewerType=all_reviews> (referer: https://www.amazon.com/Google-Pixel-Unlocked-12-3MP-Camera/product-reviews/B01M01ZZAC?pageNumber=21&reviewerType=all_reviews)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/timon/miniconda3/envs/textmining/lib/python3.5/site-packages/scrapy/utils/defer.py\", line 102, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"/home/timon/miniconda3/envs/textmining/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py\", line 30, in process_spider_output\n",
      "    for x in result:\n",
      "  File \"/home/timon/miniconda3/envs/textmining/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py\", line 339, in <genexpr>\n",
      "    return (_set_referer(r) for r in result or ())\n",
      "  File \"/home/timon/miniconda3/envs/textmining/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py\", line 37, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"/home/timon/miniconda3/envs/textmining/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py\", line 58, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"<ipython-input-9-ec6637f5de33>\", line 62, in parse\n",
      "    response.xpath('//li[@class=\"a-last\"]/a/@href').extract()[0],\n",
      "IndexError: list index out of range\n",
      "2019-03-06 11:06:02 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2019-03-06 11:06:02 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 9725,\n",
      " 'downloader/request_count': 22,\n",
      " 'downloader/request_method_count/GET': 22,\n",
      " 'downloader/response_bytes': 997108,\n",
      " 'downloader/response_count': 22,\n",
      " 'downloader/response_status_count/200': 22,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2019, 3, 6, 10, 6, 2, 183336),\n",
      " 'log_count/DEBUG': 23,\n",
      " 'log_count/ERROR': 1,\n",
      " 'log_count/INFO': 7,\n",
      " 'log_count/WARNING': 22,\n",
      " 'memusage/max': 4103401472,\n",
      " 'memusage/startup': 4103401472,\n",
      " 'request_depth_max': 21,\n",
      " 'response_received_count': 22,\n",
      " 'scheduler/dequeued': 22,\n",
      " 'scheduler/dequeued/memory': 22,\n",
      " 'scheduler/enqueued': 22,\n",
      " 'scheduler/enqueued/memory': 22,\n",
      " 'spider_exceptions/IndexError': 1,\n",
      " 'start_time': datetime.datetime(2019, 3, 6, 10, 5, 7, 891028)}\n",
      "2019-03-06 11:06:02 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "TFIDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-06 11:06:06 [gensim.corpora.dictionary] INFO: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-03-06 11:06:06 [gensim.corpora.dictionary] INFO: built Dictionary(2024 unique tokens: ['adjust', 'feature', 'specification', 'cmon', 'trust']...) from 176 documents (total 8330 corpus positions)\n",
      "2019-03-06 11:06:06 [gensim.models.tfidfmodel] INFO: collecting document frequencies\n",
      "2019-03-06 11:06:06 [gensim.models.tfidfmodel] INFO: PROGRESS: processing document #0\n",
      "2019-03-06 11:06:06 [gensim.models.tfidfmodel] INFO: calculating IDF weights for 176 documents and 2023 features (5874 matrix non-zeros)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract noun phrases...\n",
      "Extract colloactions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-06 11:06:57 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): api.cortical.io:80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create topics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-06 11:07:04 [urllib3.connectionpool] DEBUG: http://api.cortical.io:80 \"POST /rest/text/keywords?retina_name=en_associative HTTP/1.1\" 200 None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplicate phrases...\n",
      "Filter sentiments...\n",
      "Loaded model from disk\n",
      "Transform data for dash...\n",
      "Create container...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [06/Mar/2019 11:07:30] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "2019-03-06 11:07:30 [werkzeug] INFO: 127.0.0.1 - - [06/Mar/2019 11:07:30] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create callbacks...\n",
      "(<dash.dependencies.Output object at 0x7f3e90b4a780>, [<dash.dependencies.Input object at 0x7f3e90b4a748>])\n",
      "(<dash.dependencies.Output object at 0x7f3e90b4a7f0>, [<dash.dependencies.Input object at 0x7f3e90b4a7b8>])\n",
      "(<dash.dependencies.Output object at 0x7f3e90b4a898>, [<dash.dependencies.Input object at 0x7f3e90b4a860>])\n",
      "(<dash.dependencies.Output object at 0x7f3e90b4a908>, [<dash.dependencies.Input object at 0x7f3e90b4a8d0>])\n",
      "(<dash.dependencies.Output object at 0x7f3e90b4a978>, [<dash.dependencies.Input object at 0x7f3e90b4a940>])\n",
      "(<dash.dependencies.Output object at 0x7f3e90b4a9e8>, [<dash.dependencies.Input object at 0x7f3e90b4a9b0>])\n",
      "(<dash.dependencies.Output object at 0x7f3e90b4aa58>, [<dash.dependencies.Input object at 0x7f3e90b4aa20>])\n",
      "(<dash.dependencies.Output object at 0x7f3e90b4aac8>, [<dash.dependencies.Input object at 0x7f3e90b4aa90>])\n",
      "(<dash.dependencies.Output object at 0x7f3e90b4ab38>, [<dash.dependencies.Input object at 0x7f3e90b4ab00>])\n",
      "(<dash.dependencies.Output object at 0x7f3e90b4aba8>, [<dash.dependencies.Input object at 0x7f3e90b4ab70>])\n",
      "(<dash.dependencies.Output object at 0x7f3e90b4a6d8>, [<dash.dependencies.Input object at 0x7f3e90b4a710>])\n",
      "(<dash.dependencies.Output object at 0x7f3e90b4ae80>, [<dash.dependencies.Input object at 0x7f3e90b4a240>])\n",
      "(<dash.dependencies.Output object at 0x7f3e90b4aef0>, [<dash.dependencies.Input object at 0x7f3e90b4aeb8>])\n",
      "(<dash.dependencies.Output object at 0x7f3e90b4af60>, [<dash.dependencies.Input object at 0x7f3e90b4af28>])\n",
      "(<dash.dependencies.Output object at 0x7f3e90b4afd0>, [<dash.dependencies.Input object at 0x7f3e90b4af98>])\n",
      "(<dash.dependencies.Output object at 0x7f3e90b4b080>, [<dash.dependencies.Input object at 0x7f3e90b4b048>])\n",
      "(<dash.dependencies.Output object at 0x7f3e90b4b0f0>, [<dash.dependencies.Input object at 0x7f3e90b4b0b8>])\n",
      "(<dash.dependencies.Output object at 0x7f3e90b4b160>, [<dash.dependencies.Input object at 0x7f3e90b4b128>])\n",
      "(<dash.dependencies.Output object at 0x7f3e90b4a630>, [<dash.dependencies.Input object at 0x7f3e90b4a1d0>])\n",
      "(<dash.dependencies.Output object at 0x7f3e90b4b2e8>, [<dash.dependencies.Input object at 0x7f3e90c64f98>])\n",
      "(<dash.dependencies.Output object at 0x7f3e90b4b358>, [<dash.dependencies.Input object at 0x7f3e90b4b320>])\n",
      "(<dash.dependencies.Output object at 0x7f3e90b4b400>, [<dash.dependencies.Input object at 0x7f3e90b4b3c8>])\n",
      "(<dash.dependencies.Output object at 0x7f3e90b4b470>, [<dash.dependencies.Input object at 0x7f3e90b4b438>])\n",
      "(<dash.dependencies.Output object at 0x7f3e90b4b4e0>, [<dash.dependencies.Input object at 0x7f3e90b4b4a8>])\n",
      "(<dash.dependencies.Output object at 0x7f3e90b4b550>, [<dash.dependencies.Input object at 0x7f3e90b4b518>])\n",
      "(<dash.dependencies.Output object at 0x7f3e90b4ae48>, [<dash.dependencies.Input object at 0x7f3e90c64fd0>])\n",
      "(<dash.dependencies.Output object at 0x7f3e90b4a6a0>, [<dash.dependencies.Input object at 0x7f3e90b4a198>])\n",
      "(<dash.dependencies.Output object at 0x7f3e90b4bac8>, [<dash.dependencies.Input object at 0x7f3e90b4b9b0>])\n",
      "(<dash.dependencies.Output object at 0x7f3e90b4bb38>, [<dash.dependencies.Input object at 0x7f3e90b4bb00>])\n",
      "(<dash.dependencies.Output object at 0x7f3e90b4bba8>, [<dash.dependencies.Input object at 0x7f3e90b4bb70>])\n",
      "(<dash.dependencies.Output object at 0x7f3e90b4b2b0>, [<dash.dependencies.Input object at 0x7f3e90b4ad68>])\n",
      "(<dash.dependencies.Output object at 0x7f3e90b4be80>, [<dash.dependencies.Input object at 0x7f3e90b4bd68>])\n",
      "(<dash.dependencies.Output object at 0x7f3e90b4bef0>, [<dash.dependencies.Input object at 0x7f3e90b4beb8>])\n",
      "(<dash.dependencies.Output object at 0x7f3e90b4bf60>, [<dash.dependencies.Input object at 0x7f3e90b4bf28>])\n",
      "(<dash.dependencies.Output object at 0x7f3e90b4bfd0>, [<dash.dependencies.Input object at 0x7f3e90b4bf98>])\n"
     ]
    }
   ],
   "source": [
    "external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n",
    "\n",
    "# create app\n",
    "app = dash.Dash(__name__, external_stylesheets=external_stylesheets)\n",
    "app.css.append_css({\"external_url\": \"https://codepen.io/anon/pen/MzZQwp.css\"})\n",
    "app.css.append_css({\n",
    "    \"external_url\":\n",
    "    \"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\"\n",
    "})\n",
    "app.css.append_css({\n",
    "    \"external_url\":\n",
    "    \"https://codepen.io/austingreen/pen/burBc.css\"\n",
    "})\n",
    "app.scripts.config.serve_locally = True\n",
    "app.layout = html.Div([\n",
    "    link_input, html.Div(id='output-data-upload')\n",
    "])\n",
    "app.config['suppress_callback_exceptions']=True\n",
    "\n",
    "def radio_toggle(n_clicks):\n",
    "    print('AAAAAAA')\n",
    "    if n_clicks %2 != 0:\n",
    "        return {'display': 'block',\n",
    "               'border-radius': 4,\n",
    "                'margin': 2,\n",
    "                'background-color': 'lightgrey',\n",
    "                'padding': 5}\n",
    "    else:\n",
    "        return {'display': 'none'}\n",
    "    \n",
    "def create_callbacks(inputs,outputs):\n",
    "    for i, output in enumerate(outputs):\n",
    "        print((output,[inputs[i]]))\n",
    "        app.callback(output, [inputs[i]])(radio_toggle)\n",
    "\n",
    "\n",
    "@app.callback(Output('output-data-upload', 'children'),\n",
    "              [Input('submit-button', 'n_clicks')],\n",
    "              [State('link', 'value')])\n",
    "def update_output(nclicks, value):\n",
    "    print(value)\n",
    "    if value is not None:\n",
    "        print('Load methods...')\n",
    "        %run ./methods.ipynb\n",
    "        print('Crawling...')\n",
    "        %run ./crawl_amazon_reviews.ipynb\n",
    "        crawl(value)\n",
    "        reactor.run()\n",
    "        print('Preprocessing...')\n",
    "        %run ./preprocess.ipynb\n",
    "        print('TFIDF...')\n",
    "        %run ./tfidf.ipynb\n",
    "        print('Extract noun phrases...')\n",
    "        %run ./extract_noun_phrases.ipynb\n",
    "        print('Extract colloactions...')\n",
    "        %run ./extract_collocations.ipynb\n",
    "        print('Create topics...')\n",
    "        %run ./create_topics.ipynb\n",
    "        print('Deduplicate phrases...')\n",
    "        %run ./deduplicate_phrases.ipynb\n",
    "        print('Filter sentiments...')\n",
    "        %run ./filter_sentiments.ipynb\n",
    "        print('Transform data for dash...')\n",
    "        score = [y for x in df.final_scores_with_sent for y in x]\n",
    "        topic = [y for x in df.topics for y in x]\n",
    "        phrase = [y for x in df.final_phrases_with_sent for y in x]\n",
    "        score=[0 if x is None else x for x in score]\n",
    "        sentiment = [y[1] for x in df.final_sent_scores2 for y in x]\n",
    "        review = []\n",
    "        for i, tmp_review in enumerate(df.review_body):\n",
    "            for phrase1 in df.final_phrases_with_sent.iloc[i]:\n",
    "                review.append(tmp_review)\n",
    "        df_topic_scores = pd.DataFrame(data={'topic': topic, 'score': score, 'phrase': phrase,'review': review, 'sentiment': sentiment})\n",
    "        df_topic_scores.dropna()\n",
    "        #df_topic_scores.drop_duplicates()\n",
    "        #df_topic_scores = df_topic_scores[df_topic_scores.score<=0]\n",
    "        with open(topics_filename, 'rb') as f:\n",
    "            sorted_topics = pickle.load(f)\n",
    "        sorted_topics = [topic for topic in sorted_topics if topic in df_topic_scores.topic.unique()]\n",
    "        print('Create container...')\n",
    "        container = []\n",
    "        container.append(create_info_container(df,image_src,sentiment))\n",
    "        container2 = container+create_topics_container(sorted_topics,df_topic_scores)\n",
    "        print('Create callbacks...')\n",
    "        inputs = []\n",
    "        outputs = []\n",
    "        for topic in sorted_topics:\n",
    "            scores = df_topic_scores[df_topic_scores.topic == topic].score\n",
    "            topic_score = (scores-min(scores))/(max(scores)-min(scores))\n",
    "            df_topic = df_topic_scores[df_topic_scores.topic == topic]\n",
    "            df_topic = df_topic.drop_duplicates(subset='phrase', keep=\"last\")\n",
    "            df_topic = df_topic.reset_index()\n",
    "            df_neg = df_topic[df_topic.sentiment<=0.5]\n",
    "            df_pos = df_topic[df_topic.sentiment>=0.5]\n",
    "\n",
    "            df_topic1 = df_pos.sort_values(by=['score'], ascending=False)\n",
    "            df_topic2 = df_neg.sort_values(by=['score'], ascending=True)\n",
    "            if len(df_topic1)>5:\n",
    "                max_len_topic1=5\n",
    "            else:\n",
    "                max_len_topic1=len(df_topic1)\n",
    "            if len(df_topic2)>5:\n",
    "                max_len_topic2=5\n",
    "            else:\n",
    "                max_len_topic2=len(df_topic2)\n",
    "            for i in range(max_len_topic1):\n",
    "                inputs.append(Input('buttonPos-'+str(re.sub('[^a-zA-z0-9]', '', topic))+str(i), 'n_clicks'))\n",
    "                outputs.append(Output('detailsPos-'+str(re.sub('[^a-zA-z0-9]', '', topic))+str(i), 'style'))\n",
    "            for i in range(max_len_topic2):\n",
    "                inputs.append(Input('buttonNeg-'+str(re.sub('[^a-zA-z0-9]', '', topic))+str(i), 'n_clicks'))\n",
    "                outputs.append(Output('detailsNeg-'+str(re.sub('[^a-zA-z0-9]', '', topic))+str(i), 'style'))\n",
    "\n",
    "        create_callbacks(inputs,outputs)\n",
    "\n",
    "        return container2\n",
    "    #return *create_topics_container\n",
    "\n",
    "\n",
    "# create callbacks\n",
    "\n",
    "# run app localy\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=False, host='0.0.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:textmining]",
   "language": "python",
   "name": "conda-env-textmining-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
