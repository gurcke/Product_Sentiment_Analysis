{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run ./methods.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_pickle('./data/fitbitflexwirelessactivitysleepwristbandblack.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run ./tfidf.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add and remove stopwords\n",
    "stopwords = ['iphone', 'fitbit', 'alta', 'fit bit', 'hr', 'sony', 'mdr', 'mdr7506', 'product', 'device', 'thing', 'love','daughter','son','husband','wife','mother','father','star','pair','thing','product','mom','dad','day','week','month','year','minute','second']\n",
    "no_stopwords = ['no', 'not']\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "for stopword in stopwords:\n",
    "    stopword_list.append(stopword)\n",
    "for no_stopword in no_stopwords:\n",
    "    stopword_list.remove(no_stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract noun chunks from sentences and clear\n",
    "review_noun_phrases = []\n",
    "for review in df.preprocessed:\n",
    "    \n",
    "    # split reviews into sentences and extract noun chunks\n",
    "    review_noun_chunks = [\n",
    "        nlp(review).noun_chunks for review in sent_tokenize(review)\n",
    "    ]\n",
    "\n",
    "    # convert noun chunks to string and remove single word noun phrases\n",
    "    review_noun_chunks_corpus = []\n",
    "    for review_noun_chunk in review_noun_chunks:\n",
    "        tmp_noun_chunks = []\n",
    "        for chunk in review_noun_chunk:\n",
    "            if len(nltk.word_tokenize(chunk.text)) > 1:\n",
    "                tmp_noun_chunks.append(chunk.text)\n",
    "        if tmp_noun_chunks:\n",
    "            review_noun_chunks_corpus = review_noun_chunks_corpus + tmp_noun_chunks\n",
    "\n",
    "    # preprocess noun phrases\n",
    "    preprocessed_noun_chunks = [\n",
    "        preprocess(\n",
    "            noun_chunk,\n",
    "            isolate_special_characters=True,\n",
    "            remove_special_characters=True,\n",
    "            remove_extra_whitespace=True,\n",
    "            remove_stopwords=True,\n",
    "            remove_numbers=True) for noun_chunk in review_noun_chunks_corpus\n",
    "    ]\n",
    "\n",
    "    # remove single word noun phrases\n",
    "    sentence_noun_phrases = []\n",
    "    for sentence in preprocessed_noun_chunks:\n",
    "        if len(nltk.word_tokenize(sentence)) > 1:\n",
    "            sentence_noun_phrases.append(sentence)\n",
    "\n",
    "    review_noun_phrases.append(sentence_noun_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_noun_phrases_nouns= []\n",
    "for review in review_noun_phrases:\n",
    "    tmp_noun_phrases_entities = []\n",
    "    for noun_chunk in review:\n",
    "        tmp_noun_phrases_entities.append(extract_entity(noun_chunk))\n",
    "    review_noun_phrases_nouns.append(tmp_noun_phrases_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = list(itertools.chain(*review_noun_phrases_nouns))\n",
    "merged2 = list(itertools.chain(*review_noun_phrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(merged2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "for i,review in enumerate(review_noun_phrases_nouns):\n",
    "    tmp_topic = []\n",
    "    for j,ph in enumerate(review):\n",
    "        try:\n",
    "            tmp_topic.append((sum([d[preprocess(word, lemmatize=True)]for word in ph.split()])/len(ph.split()))*len(review_noun_phrases[i][j].split()))\n",
    "        except:\n",
    "            tmp_topic.append(None)\n",
    "    score.append(tmp_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"noun_phrases\"] = review_noun_phrases\n",
    "df[\"noun_phrase_scores\"] = score\n",
    "df[\"noun_phrase_entities\"] = review_noun_phrases_nouns\n",
    "#df.to_pickle('./data/fitbitflexwirelessactivitysleepwristbandblack.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten1 = list(itertools.chain(*review_noun_phrases))\n",
    "#flatten2 = list(itertools.chain(*score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d = {'np': flatten1, 'score': flatten2}\n",
    "#lel = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lel = lel.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lel.sort_values(by=['score'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:textmining]",
   "language": "python",
   "name": "conda-env-textmining-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
