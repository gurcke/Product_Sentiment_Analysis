{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import scrapy\n",
    "import re\n",
    "import dateparser\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import unicodedata\n",
    "import nltk\n",
    "import re\n",
    "import textacy\n",
    "import contractions\n",
    "import itertools\n",
    "import json\n",
    "import retinasdk\n",
    "import math\n",
    "import nltk\n",
    "import pyLDAvis.gensim\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from nltk import pos_tag\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from gensim import corpora, models\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from keras.models import model_from_json\n",
    "from scipy import spatial\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import tokenize\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from similarity.jarowinkler import JaroWinkler\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec # the word2vec model gensim class\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "liteClient = retinasdk.LiteClient(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add and remove stopwords\n",
    "stopwords = ['iphone', 'fitbit', 'alta', 'fit bit', 'hr', 'sony', 'mdr', 'mdr7506', 'product', 'device', 'thing', 'love','daughter','son','husband','wife','mother','father','star','pair','thing','product']\n",
    "no_stopwords = ['no', 'not']\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "for stopword in stopwords:\n",
    "    stopword_list.append(stopword)\n",
    "for no_stopword in no_stopwords:\n",
    "    stopword_list.remove(no_stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg', parse=False, tag=False, entity=False)\n",
    "tokenizer = ToktokTokenizer()\n",
    "def preprocess(review,\n",
    "               tokenize=False,\n",
    "               links=False,\n",
    "               html_tags=False,\n",
    "               accented_chars=False,\n",
    "               expand_contractions=False,\n",
    "               lowercase=False,\n",
    "               remove_newlines=False,\n",
    "               isolate_special_characters=False,\n",
    "               remove_word_types=[],\n",
    "               remove_numbers=False,\n",
    "               lemmatize=False,\n",
    "               remove_special_characters=False,\n",
    "               remove_extra_whitespace=False,\n",
    "               remove_stopwords=False):\n",
    "\n",
    "    # remove links\n",
    "    if links:\n",
    "        review = re.sub(\n",
    "            r'^https?:\\/\\/.*[\\r\\n]*', '', review, flags=re.MULTILINE)\n",
    "    # remove html tags\n",
    "    if html_tags:\n",
    "        soup = BeautifulSoup(review, \"html.parser\")\n",
    "        review = soup.get_text()\n",
    "    # remove accented characters\n",
    "    if accented_chars:\n",
    "        review = unicodedata.normalize('NFKD', review).encode(\n",
    "            'ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    # expand contractions\n",
    "    if expand_contractions:\n",
    "        review = contractions.fix(review)\n",
    "    # lowercase the text\n",
    "    if lowercase:\n",
    "        review = review.lower()\n",
    "    # remove extra newlines\n",
    "    if remove_newlines:\n",
    "        review = re.sub(r'[\\r|\\n|\\r\\n]+', ' ', review)\n",
    "    # insert spaces between special characters to isolate them\n",
    "    if isolate_special_characters:\n",
    "        #special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "        special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "        review = special_char_pattern.sub(\" \\\\1 \", review)\n",
    "    # lemmatize text\n",
    "    if lemmatize:\n",
    "        review = ' '.join([\n",
    "            str(word.lemma_) if word.lemma_ != '-PRON-' else str(word.text)\n",
    "            for word in nlp(review)\n",
    "        ])\n",
    "    # remove word types\n",
    "    if remove_word_types:\n",
    "        tokens = tokenizer.tokenize(review)\n",
    "        review = ' '.join([\n",
    "            str(token) for token in tokens\n",
    "            if nlp(token)[0].pos_ not in remove_word_types\n",
    "        ])\n",
    "    # remove numbers\n",
    "    if remove_numbers:\n",
    "        review = re.sub('[0-9]', '', review)\n",
    "    # remove special characters\n",
    "    if remove_special_characters:\n",
    "        review = re.sub('[^a-zA-z0-9\\s]', '', review)\n",
    "    # remove extra whitespace\n",
    "    if remove_extra_whitespace:\n",
    "        review = re.sub(' +', ' ', review)\n",
    "    # remove stopwords\n",
    "    if remove_stopwords:\n",
    "        review = ' '.join([\n",
    "            token.strip() for token in tokenizer.tokenize(review)\n",
    "            if token not in stopword_list\n",
    "        ])\n",
    "    if tokenize:\n",
    "        review = [token.strip() for token in tokenizer.tokenize(review)]\n",
    "\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains(string1, string2):\n",
    "    string1_lemma = preprocess(\n",
    "        string1[0],\n",
    "        lemmatize=True)\n",
    "    string2_lemma = preprocess(\n",
    "        string2[0],\n",
    "        lemmatize=True)\n",
    "    string1_len = len(preprocess(string1_lemma,tokenize=True))\n",
    "    string2_len = len(preprocess(string2_lemma,tokenize=True))\n",
    "    keep = []\n",
    "    if(string1_len>string2_len):\n",
    "        if string2_lemma in string1_lemma:\n",
    "            keep.append([string1[0],string1[1],string1[2]])\n",
    "        else:\n",
    "            keep.append([string1[0],string1[1],string1[2]])\n",
    "            keep.append([string2[0],string2[1],string2[2]])\n",
    "    elif(string2_len>string1_len):\n",
    "        if string1_lemma in string2_lemma:\n",
    "            keep.append([string2[0],string2[1],string2[2]])\n",
    "        else:\n",
    "            keep.append([string1[0],string1[1],string1[2]])\n",
    "            keep.append([string2[0],string2[1],string2[2]])\n",
    "    elif(string1_lemma==string2_lemma):\n",
    "        keep.append([string1[0],string1[1],string1[2]])\n",
    "    else:\n",
    "        keep.append([string1[0],string1[1],string1[2]])\n",
    "        keep.append([string2[0],string2[1],string2[2]])\n",
    "    return keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg', parse=False, tag=False, entity=False)\n",
    "tokenizer = ToktokTokenizer()\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "jarowinkler = JaroWinkler()\n",
    "review_w2v = KeyedVectors.load(\"data/reviews_300dim_10minwords_10context\", mmap='r')\n",
    "def get_phrase_vec(phrase):\n",
    "    phrase_vec = np.zeros(300)\n",
    "    for word in tokenizer.tokenize(phrase):\n",
    "        phrase_vec += review_w2v.get_vector(word)\n",
    "    phrase_vec/=len(phrase)\n",
    "    return phrase_vec\n",
    "def cosinus_distance(a,b):\n",
    "    return 1 - spatial.distance.cosine(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_similarity(s1, s2, normalize=True):\n",
    "\n",
    "    s1_new = preprocess(\n",
    "        s1,\n",
    "        lemmatize=True,\n",
    "        remove_special_characters=True,\n",
    "        remove_extra_whitespace=True)\n",
    "    \n",
    "    s2_new = preprocess(\n",
    "        s1,\n",
    "        lemmatize=True,\n",
    "        remove_special_characters=True,\n",
    "        remove_extra_whitespace=True)\n",
    "\n",
    "    return([s2, s1, jarowinkler.similarity(s1_new, s2_new)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "def extract_entity(s1):\n",
    "    s1_new = preprocess(\n",
    "    s1,\n",
    "    remove_stopwords=True,\n",
    "    lemmatize=True,\n",
    "    remove_extra_whitespace=True,\n",
    "    remove_numbers=True)\n",
    "    remove_word_types=[\n",
    "            'PART', 'VERB', 'PRON', 'DET', 'CONJ', 'INTJ', 'NUM',\n",
    "            'ADP', 'CCONJ', 'ADJ', 'ADV'\n",
    "        ]\n",
    "    tokens = tokenizer.tokenize(s1_new)\n",
    "    s1_new = ' '.join([\n",
    "        str(token) for token in tokens\n",
    "        if nlp(token)[0].pos_ not in remove_word_types\n",
    "    ])\n",
    "\n",
    "    s1_new = ' '.join([word for word in s1_new.split() if word not in sid.lexicon ])\n",
    "    return(s1_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:textmining]",
   "language": "python",
   "name": "conda-env-textmining-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
